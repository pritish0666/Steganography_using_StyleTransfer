{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pritish0666/Steganography_using_StyleTransfer/blob/master/FINAL_MAJOR_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXz2kezPs6O4",
        "outputId": "c643e424-fcd7-43cb-e4d4-466666495fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Folder found! ✔\n",
            "Path: /content/drive/My Drive/STEGO_MAJOR_PROJECT/scraped_images_640_yolo\n",
            "Files inside:\n",
            "['data.yaml', 'val', 'train']\n"
          ]
        }
      ],
      "source": [
        "#cell Negative\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. SET YOUR FOLDER PATH HERE\n",
        "# Replace the folder name EXACTLY as it appears in Drive\n",
        "folder_path = \"/content/drive/My Drive/STEGO_MAJOR_PROJECT/scraped_images_640_yolo\"\n",
        "\n",
        "# 3. Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    print(\"Folder found! ✔\")\n",
        "    print(\"Path:\", folder_path)\n",
        "    print(\"Files inside:\")\n",
        "    print(os.listdir(folder_path))\n",
        "else:\n",
        "    print(\"❌ Folder NOT found. Check the path again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3dMN0BHsxG8",
        "outputId": "d2c71340-2013-4d56-d8d9-d1b7650fefda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DATA_ROOT is set to: /content/drive/MyDrive/STEGO_MAJOR_PROJECT/scraped_images_640_yolo\n",
            "Top-level in DATA_ROOT: ['data.yaml', 'val', 'train']\n",
            "train/images count: 464\n",
            "train/labels count: 464\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/STEGO_MAJOR_PROJECT/scraped_images_640_yolo\"\n",
        "\n",
        "def is_dataset_root(p):\n",
        "    return os.path.isdir(os.path.join(p,\"train\",\"images\")) and os.path.isdir(os.path.join(p,\"val\",\"images\"))\n",
        "\n",
        "if not os.path.exists(DATA_ROOT):\n",
        "    raise FileNotFoundError(f\"DATA_ROOT not found: {DATA_ROOT}. Upload your folder to Colab and update DATA_ROOT.\")\n",
        "\n",
        "if not is_dataset_root(DATA_ROOT):\n",
        "    raise RuntimeError(f\"Data root missing required train/images & val/images folders: {DATA_ROOT}\")\n",
        "\n",
        "print(\"✅ DATA_ROOT is set to:\", DATA_ROOT)\n",
        "print(\"Top-level in DATA_ROOT:\", os.listdir(DATA_ROOT))\n",
        "print(\"train/images count:\", len(os.listdir(os.path.join(DATA_ROOT,\"train\",\"images\"))))\n",
        "print(\"train/labels count:\", len(os.listdir(os.path.join(DATA_ROOT,\"train\",\"labels\"))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK4d-8e6s25O",
        "outputId": "11118d1d-52d3-4f79-8513-d176e63fd8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os, math, random, time\n",
        "from glob import glob\n",
        "from collections import defaultdict\n",
        "from PIL import Image, ImageDraw\n",
        "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "\n",
        "OUTPUTS_DIR = \"/content/outputs\"\n",
        "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDet6gUwKP0",
        "outputId": "8c352f29-5e31-40f1-d8bb-87c49bd77852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found label files: 464\n",
            "class  0 -> 27 entries\n",
            "class  1 -> 28 entries\n",
            "class  2 -> 31 entries\n",
            "class  3 -> 29 entries\n",
            "class  4 -> 28 entries\n",
            "class  5 -> 28 entries\n",
            "class  6 -> 28 entries\n",
            "class  7 -> 30 entries\n",
            "class  8 -> 28 entries\n",
            "class  9 -> 28 entries\n",
            "class 10 -> 28 entries\n",
            "class 11 -> 32 entries\n",
            "class 12 -> 29 entries\n",
            "class 13 -> 30 entries\n",
            "class 14 -> 30 entries\n",
            "class 15 -> 30 entries\n"
          ]
        }
      ],
      "source": [
        "TRAIN_IMG_DIR = os.path.join(DATA_ROOT, \"train\", \"images\")\n",
        "TRAIN_LBL_DIR = os.path.join(DATA_ROOT, \"train\", \"labels\")\n",
        "\n",
        "def img_for_label(lbl_path):\n",
        "    stem = os.path.splitext(os.path.basename(lbl_path))[0]\n",
        "    for ext in (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"):\n",
        "        p = os.path.join(TRAIN_IMG_DIR, stem + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "class_to_entries = defaultdict(list)\n",
        "lbl_files = sorted(glob(os.path.join(TRAIN_LBL_DIR, \"*.txt\")))\n",
        "print(\"Found label files:\", len(lbl_files))\n",
        "\n",
        "for lf in lbl_files:\n",
        "    img_path = img_for_label(lf)\n",
        "    if not img_path:\n",
        "        continue\n",
        "    W, H = Image.open(img_path).size\n",
        "    boxes_by_class = defaultdict(list)\n",
        "    with open(lf, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5: continue\n",
        "            cid = int(parts[0])\n",
        "            cx, cy, w, h = map(float, parts[1:5])\n",
        "            xmin = max(0, int((cx - w/2) * W))\n",
        "            ymin = max(0, int((cy - h/2) * H))\n",
        "            xmax = min(W, int((cx + w/2) * W))\n",
        "            ymax = min(H, int((cy + h/2) * H))\n",
        "            if xmax > xmin and ymax > ymin:\n",
        "                boxes_by_class[cid].append((xmin, ymin, xmax, ymax))\n",
        "    for cid, bboxes in boxes_by_class.items():\n",
        "        class_to_entries[cid].append((img_path, bboxes))\n",
        "\n",
        "for cid in range(16):\n",
        "    print(f\"class {cid:2d} -> {len(class_to_entries[cid])} entries\")\n",
        "\n",
        "missing = [cid for cid in range(16) if len(class_to_entries[cid]) == 0]\n",
        "if missing:\n",
        "    print(\"WARNING: missing classes:\", missing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S4_nXDuWwNSF"
      },
      "outputs": [],
      "source": [
        "hex_digits = [hex(i)[2:].upper() for i in range(16)]\n",
        "hex_to_cid = {h:i for i,h in enumerate(hex_digits)}\n",
        "\n",
        "def load_tile_for_class(cid, tile_size=128):\n",
        "    entries = class_to_entries.get(cid, [])\n",
        "    if entries:\n",
        "        img_path, bboxes = random.choice(entries)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if bboxes:\n",
        "            xmin,ymin,xmax,ymax = random.choice(bboxes)\n",
        "            tile = img.crop((xmin,ymin,xmax,ymax))\n",
        "        else:\n",
        "            W,H = img.size\n",
        "            side = min(W,H); left=(W-side)//2; top=(H-side)//2\n",
        "            tile = img.crop((left,top,left+side,top+side))\n",
        "        return tile.resize((tile_size,tile_size), Image.BICUBIC)\n",
        "    all_imgs = sorted(glob(os.path.join(TRAIN_IMG_DIR, \"*.*\")))\n",
        "    if all_imgs:\n",
        "        img = Image.open(random.choice(all_imgs)).convert(\"RGB\")\n",
        "        W,H = img.size; side=min(W,H); left=(W-side)//2; top=(H-side)//2\n",
        "        return img.crop((left,top,left+side,top+side)).resize((tile_size,tile_size), Image.BICUBIC)\n",
        "    t = Image.new(\"RGB\",(tile_size,tile_size),(240,240,240)); ImageDraw.Draw(t).text((8,8), str(cid), fill=(0,0,0)); return t\n",
        "\n",
        "def text_to_hex(text):\n",
        "    return \"\".join(f\"{ord(c):02X}\" for c in text)\n",
        "\n",
        "def make_grid_from_hex(hex_str, grid_size=None, tile_size=128):\n",
        "    digits = list(hex_str)\n",
        "    if grid_size is None:\n",
        "        grid_size = math.ceil(math.sqrt(len(digits)))\n",
        "    total = grid_size*grid_size\n",
        "    if len(digits) < total:\n",
        "        digits += ['0']*(total-len(digits))\n",
        "    canvas = Image.new(\"RGB\",(grid_size*tile_size, grid_size*tile_size),(255,255,255))\n",
        "    for idx,d in enumerate(digits[:total]):\n",
        "        r,c = divmod(idx, grid_size)\n",
        "        cid = hex_to_cid.get(d.upper(),0)\n",
        "        tile = load_tile_for_class(cid, tile_size=tile_size)\n",
        "        canvas.paste(tile, (c*tile_size, r*tile_size))\n",
        "    return canvas\n",
        "\n",
        "def secret_image_to_hex(secret_img_path, resize=(16,16)):\n",
        "    img = Image.open(secret_img_path).convert(\"RGB\")\n",
        "    img_small = img.resize(resize, Image.BICUBIC)\n",
        "    data = img_small.tobytes()\n",
        "    hex_str = \"\".join(f\"{b:02X}\" for b in data)\n",
        "    return hex_str, img_small\n",
        "\n",
        "def make_dataset_grid_from_secret_image(secret_img_path, resize=(16,16), tile_size=64, grid_size=None):\n",
        "    hex_str, small = secret_image_to_hex(secret_img_path, resize=resize)\n",
        "    grid = make_grid_from_hex(hex_str, grid_size=grid_size, tile_size=tile_size)\n",
        "    return grid, hex_str, small\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q6VsUjJw5x3",
        "outputId": "0d99207f-c745-4761-a19a-0ad3f572aa86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548M/548M [00:07<00:00, 79.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "def replace_relu_with_outplace(module):\n",
        "    for name, layer in module.named_children():\n",
        "        if isinstance(layer, nn.ReLU):\n",
        "            setattr(module, name, nn.ReLU(inplace=False))\n",
        "        else:\n",
        "            replace_relu_with_outplace(layer)\n",
        "\n",
        "cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
        "replace_relu_with_outplace(cnn)\n",
        "\n",
        "def image_loader(path, imsize=256):\n",
        "    loader = transforms.Compose([transforms.Resize((imsize,imsize)), transforms.ToTensor()])\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    t = loader(img).unsqueeze(0).to(device, torch.float)\n",
        "    return t\n",
        "\n",
        "def im_to_pil(tensor):\n",
        "    t = tensor.clone().detach().cpu().squeeze(0)\n",
        "    return transforms.ToPILImage()(torch.clamp(t,0,1))\n",
        "\n",
        "def gram_matrix(x):\n",
        "    b,ch,h,w = x.size()\n",
        "    f = x.view(b,ch,h*w)\n",
        "    return torch.bmm(f, f.transpose(1,2)) / (ch*h*w)\n",
        "\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super().__init__()\n",
        "        self.target = target.detach(); self.loss = 0\n",
        "    def forward(self,x):\n",
        "        self.loss = F.mse_loss(x, self.target); return x\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self,target):\n",
        "        super().__init__()\n",
        "        self.target = gram_matrix(target).detach(); self.loss = 0\n",
        "    def forward(self,x):\n",
        "        self.loss = F.mse_loss(gram_matrix(x), self.target); return x\n",
        "\n",
        "def get_model_and_losses(cnn, style_img, content_img):\n",
        "    content_layers = ['21']\n",
        "    style_layers = ['0','5','10','19','28']\n",
        "    model = nn.Sequential().to(device)\n",
        "    style_losses, content_losses = [], []\n",
        "    i=0\n",
        "    for layer in cnn.children():\n",
        "        model.add_module(str(i), layer)\n",
        "        if str(i) in style_layers:\n",
        "            target = model(style_img).detach(); sl = StyleLoss(target); model.add_module(\"style_loss_\"+str(i), sl); style_losses.append(sl)\n",
        "        if str(i) in content_layers:\n",
        "            target = model(content_img).detach(); cl = ContentLoss(target); model.add_module(\"content_loss_\"+str(i), cl); content_losses.append(cl)\n",
        "        i+=1\n",
        "    return model, style_losses, content_losses\n",
        "\n",
        "def generate_stego(secret_path, cover_path, out_path, imsize=256, alpha=0.1, beta=4e4, steps=200):\n",
        "    content_img = image_loader(secret_path, imsize)\n",
        "    style_img = image_loader(cover_path, imsize)\n",
        "    input_img = style_img.clone()\n",
        "    model, style_losses, content_losses = get_model_and_losses(cnn, style_img, content_img)\n",
        "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "    run=[0]\n",
        "    while run[0] <= steps:\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            input_img.data.clamp_(0,1)\n",
        "            model(input_img)\n",
        "            style_score = sum(sl.loss for sl in style_losses)\n",
        "            content_score = sum(cl.loss for cl in content_losses)\n",
        "            loss = alpha*content_score + beta*style_score\n",
        "            loss.backward()\n",
        "            run[0]+=1\n",
        "            if run[0] % 20 == 0 or run[0]==steps:\n",
        "                print(f\"Step {run[0]} | Content {content_score.item():.4e} | Style {style_score.item():.4e}\")\n",
        "            return loss\n",
        "        optimizer.step(closure)\n",
        "    input_img.data.clamp_(0,1)\n",
        "    out = im_to_pil(input_img); out.save(out_path)\n",
        "    print(\"✅ Saved stego to:\", out_path)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "PlosXVJT6LY3",
        "outputId": "561719f4-52b6-47c1-ef9c-bf931252e5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose mode:\n",
            "  1) Text → Stego\n",
            "  2) Image → Stego\n",
            "\n",
            "Enter 1 or 2: 2\n",
            "Upload SECRET image file now (it will be used as secret content):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3365cd7e-55f8-4a3c-9251-ad568389c16f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3365cd7e-55f8-4a3c-9251-ad568389c16f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving mountain.jpg to mountain.jpg\n",
            "Saved a copy of the uploaded secret to: /content/outputs/secret_uploaded_1763867345_mountain.jpg\n",
            "Grid size (enter for auto): 32\n",
            "Converting secret image -> hex -> dataset grid...\n",
            "Secret grid saved: /content/outputs/secret_image_grid_1763867360.png\n",
            "Small preview saved: /content/outputs/secret_small_preview_1763867362.png\n",
            "\n",
            "Upload COVER image now (it will be used as cover/style):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f9c15bf-166f-4d0f-b3d2-408b8ce0bc9f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f9c15bf-166f-4d0f-b3d2-408b8ce0bc9f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving snow.jpg to snow.jpg\n",
            "Saved cover copy to: /content/outputs/cover_1763867372_snow.jpg\n",
            "Content weight alpha (default 0.1): \n",
            "Style weight beta (default 40000): \n",
            "\n",
            "Running style-transfer embedding...\n",
            "\n",
            "Step 20 | Content 4.9389e+01 | Style 9.0070e-06\n",
            "Step 40 | Content 4.3083e+01 | Style 1.2250e-05\n",
            "Step 60 | Content 4.0628e+01 | Style 1.3027e-05\n",
            "✅ Saved stego to: /content/outputs/stego_1763867375.png\n",
            "\n",
            "Done. Stego saved at: /content/outputs/stego_1763867375.png\n",
            "All related files (secret, cover, preview) are kept in: /content/outputs\n"
          ]
        }
      ],
      "source": [
        "# # === Cell 5 (corrected) — Main interactive flow (saves uploaded files to OUTPUTS_DIR) ===\n",
        "# import os, time, shutil\n",
        "# from google.colab import files\n",
        "\n",
        "# print(\"Choose mode:\\n  1) Text → Stego (text uses dataset tiles)\\n  2) Image → Stego (secret image -> hex -> dataset tiles)\\n\")\n",
        "# mode = input(\"Enter 1 or 2: \").strip()\n",
        "\n",
        "# # Ensure outputs dir exists\n",
        "# os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
        "\n",
        "# if mode == \"1\":\n",
        "#     text = input(\"Enter text to hide (keep reasonably short): \").strip()\n",
        "#     grid_size_input = input(\"Grid size (enter for auto): \").strip()\n",
        "#     tile_size = int(input(\"Tile size in px (recommend 64 or 128): \").strip() or \"128\")\n",
        "#     grid_size = int(grid_size_input) if grid_size_input else None\n",
        "\n",
        "import os, time, shutil\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Choose mode:\\n  1) Text → Stego\\n  2) Image → Stego\\n\")\n",
        "mode = input(\"Enter 1 or 2: \").strip()\n",
        "\n",
        "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
        "\n",
        "tile_size = 64\n",
        "default_resize = (16,16)\n",
        "default_steps = 40\n",
        "default_imsize = 256\n",
        "\n",
        "if mode == \"1\":\n",
        "    text = input(\"Enter text to hide (keep reasonably short): \").strip()\n",
        "    grid_size_input = input(\"Grid size (enter for auto): \").strip()\n",
        "    grid_size = int(grid_size_input) if grid_size_input else None\n",
        "    secret_grid = make_grid_from_hex(text_to_hex(text), grid_size=grid_size, tile_size=tile_size)\n",
        "    secret_basename = f\"secret_text_grid_{int(time.time())}.png\"\n",
        "    secret_path = os.path.join(OUTPUTS_DIR, secret_basename)\n",
        "    secret_grid.save(secret_path)\n",
        "    print(\"Secret grid saved:\", secret_path)\n",
        "\n",
        "elif mode == \"2\":\n",
        "    print(\"Upload SECRET image file now (it will be used as secret content):\")\n",
        "    uploaded = files.upload()\n",
        "    sec_fp = list(uploaded.keys())[0]\n",
        "    secret_input_path = os.path.join(\"/content\", sec_fp)\n",
        "    saved_secret_fp = f\"secret_uploaded_{int(time.time())}_{sec_fp}\"\n",
        "    saved_secret_path = os.path.join(OUTPUTS_DIR, saved_secret_fp)\n",
        "    shutil.copy2(secret_input_path, saved_secret_path)\n",
        "    print(\"Saved a copy of the uploaded secret to:\", saved_secret_path)\n",
        "\n",
        "    grid_size_input = input(\"Grid size (enter for auto): \").strip()\n",
        "    grid_size = int(grid_size_input) if grid_size_input else None\n",
        "\n",
        "    print(\"Converting secret image -> hex -> dataset grid...\")\n",
        "    secret_grid, hex_str, small_preview = make_dataset_grid_from_secret_image(\n",
        "        secret_input_path, resize=default_resize, tile_size=tile_size, grid_size=grid_size\n",
        "    )\n",
        "\n",
        "    secret_basename = f\"secret_image_grid_{int(time.time())}.png\"\n",
        "    secret_path = os.path.join(OUTPUTS_DIR, secret_basename)\n",
        "    secret_grid.save(secret_path)\n",
        "\n",
        "    preview_basename = f\"secret_small_preview_{int(time.time())}.png\"\n",
        "    preview_path = os.path.join(OUTPUTS_DIR, preview_basename)\n",
        "    small_preview.save(preview_path)\n",
        "\n",
        "    print(\"Secret grid saved:\", secret_path)\n",
        "    print(\"Small preview saved:\", preview_path)\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Invalid mode selected. Enter '1' or '2'\")\n",
        "\n",
        "print(\"\\nUpload COVER image now (it will be used as cover/style):\")\n",
        "uploaded = files.upload()\n",
        "cover_fp = list(uploaded.keys())[0]\n",
        "cover_temp_path = os.path.join(\"/content\", cover_fp)\n",
        "cover_saved_name = f\"cover_{int(time.time())}_{cover_fp}\"\n",
        "cover_path = os.path.join(OUTPUTS_DIR, cover_saved_name)\n",
        "shutil.copy2(cover_temp_path, cover_path)\n",
        "print(\"Saved cover copy to:\", cover_path)\n",
        "\n",
        "alpha = float(input(\"Content weight alpha (default 0.1): \").strip() or \"0.1\")\n",
        "beta = float(input(\"Style weight beta (default 40000): \").strip() or \"40000\")\n",
        "\n",
        "out_name = f\"stego_{int(time.time())}.png\"\n",
        "out_path = os.path.join(OUTPUTS_DIR, out_name)\n",
        "print(\"\\nRunning style-transfer embedding...\\n\")\n",
        "\n",
        "stego_img = generate_stego(secret_path, cover_path, out_path, imsize=default_imsize, alpha=alpha, beta=beta, steps=default_steps)\n",
        "\n",
        "print(\"\\nDone. Stego saved at:\", out_path)\n",
        "print(\"All related files (secret, cover, preview) are kept in:\", OUTPUTS_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nN8Yi4fDxC5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6deeda2b-715d-4904-b9d0-4d2e036588f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save outputs to Google Drive? (y/n): n\n",
            "Skipped saving to Drive.\n"
          ]
        }
      ],
      "source": [
        "1# Cell 6 — Save outputs to Google Drive (optional)\n",
        "save_drive = input(\"Save outputs to Google Drive? (y/n): \").strip().lower()\n",
        "if save_drive == \"y\":\n",
        "    from google.colab import drive, files\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_DIR = \"/content/drive/MyDrive/StegoOutputs\"\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    # copy all files in OUTPUTS_DIR\n",
        "    import shutil\n",
        "    for fn in os.listdir(OUTPUTS_DIR):\n",
        "        src = os.path.join(OUTPUTS_DIR, fn)\n",
        "        dst = os.path.join(DRIVE_DIR, fn)\n",
        "        shutil.copy2(src, dst)\n",
        "    print(\"Saved outputs to:\", DRIVE_DIR)\n",
        "    print(\"Listing saved files:\")\n",
        "    print(os.listdir(DRIVE_DIR)[-20:])\n",
        "else:\n",
        "    print(\"Skipped saving to Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7HCrwPmW0IE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f02177-5b7c-44af-cb10-b6049c23272b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (0.24.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.3)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-image lpips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0batbWAM4QpO",
        "outputId": "778ee9bc-3dd8-4b05-a217-47598679f7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 175MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Image size used for metrics (HxW): (523, 860)\n",
            "                     \n",
            "SSIM:  0.7638   (1.0 = identical)\n",
            "PSNR:  31.24 dB\n",
            "LPIPS: 0.5447   (0 = perceptually identical)\n"
          ]
        }
      ],
      "source": [
        "import os, cv2, numpy as np, torch, lpips\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "cover_path = \"/content/outputs/cover_1763867372_snow.jpg\"\n",
        "stego_path = \"/content/outputs/stego_1763867375.png\"\n",
        "cover_bgr = cv2.imread(cover_path, cv2.IMREAD_UNCHANGED)\n",
        "stego_bgr = cv2.imread(stego_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "if cover_bgr is None:\n",
        "    raise FileNotFoundError(f\"Cover not found: {cover_path}\")\n",
        "if stego_bgr is None:\n",
        "    raise FileNotFoundError(f\"Stego not found: {stego_path}\")\n",
        "\n",
        "if cover_bgr.ndim == 3 and cover_bgr.shape[2] == 4:\n",
        "    cover_bgr = cv2.cvtColor(cover_bgr, cv2.COLOR_BGRA2BGR)\n",
        "if stego_bgr.ndim == 3 and stego_bgr.shape[2] == 4:\n",
        "    stego_bgr = cv2.cvtColor(stego_bgr, cv2.COLOR_BGRA2BGR)\n",
        "\n",
        "cover_rgb = cv2.cvtColor(cover_bgr, cv2.COLOR_BGR2RGB)\n",
        "stego_rgb  = cv2.cvtColor(stego_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "Hc, Wc = cover_rgb.shape[:2]\n",
        "Hs, Ws = stego_rgb.shape[:2]\n",
        "if (Hc, Wc) != (Hs, Ws):\n",
        "    stego_rgb = cv2.resize(stego_rgb, (Wc, Hc), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "MIN_SIDE = 64\n",
        "Hc, Wc = cover_rgb.shape[:2]\n",
        "if min(Hc, Wc) < MIN_SIDE:\n",
        "    new_w = max(MIN_SIDE, Wc)\n",
        "    new_h = max(MIN_SIDE, Hc)\n",
        "    cover_rgb = cv2.resize(cover_rgb, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
        "    stego_rgb = cv2.resize(stego_rgb, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
        "    Hc, Wc = new_h, new_w\n",
        "    print(f\"Upsampled images to ({Hc},{Wc}) to satisfy SSIM window size requirements.\")\n",
        "\n",
        "cover_arr = np.asarray(cover_rgb).astype(np.float32)\n",
        "stego_arr  = np.asarray(stego_rgb).astype(np.float32)\n",
        "\n",
        "try:\n",
        "    ssim_score = ssim(cover_arr, stego_arr, data_range=255.0, channel_axis=2)\n",
        "except Exception as e:\n",
        "    print(\"SSIM failed on RGB with error:\", e)\n",
        "    cover_gray = cv2.cvtColor(cover_rgb, cv2.COLOR_RGB2GRAY)\n",
        "    stego_gray = cv2.cvtColor(stego_rgb, cv2.COLOR_RGB2GRAY)\n",
        "    ssim_score = ssim(cover_gray, stego_gray, data_range=255.0)\n",
        "\n",
        "psnr_score = psnr(cover_arr, stego_arr, data_range=255.0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "def to_lpips_tensor(img_rgb):\n",
        "    t = torch.from_numpy(img_rgb.astype(np.float32) / 255.0).permute(2,0,1).unsqueeze(0)\n",
        "    t = t * 2.0 - 1.0\n",
        "    return t.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    a = to_lpips_tensor(cover_rgb)\n",
        "    b = to_lpips_tensor(stego_rgb)\n",
        "    lpips_score = loss_fn(a, b).item()\n",
        "\n",
        "print(\"Image size used for metrics (HxW):\", (Hc, Wc))\n",
        "print(\"                     \")\n",
        "print(f\"SSIM:  {ssim_score:.4f}   (1.0 = identical)\")\n",
        "print(f\"PSNR:  {psnr_score:.2f} dB\")\n",
        "print(f\"LPIPS: {lpips_score:.4f}   (0 = perceptually identical)\")\n",
        "\n",
        "# print(\"\\nGuidance: SSIM >0.9 & PSNR >35 dB & LPIPS <0.2 -> very high visual similarity.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lOeaxfs48XV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdArGbt4Hjaq"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOTFyDGTbMqCyGkIRgikD4C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}